
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Segunda quest?o}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Questão 2 - Prova 2 de Inteligência
Artificial}\label{questuxe3o-2---prova-2-de-inteliguxeancia-artificial}

\subsection{Lucas Nóbrega e Nathália de
Vasconcelos}\label{lucas-nuxf3brega-e-nathuxe1lia-de-vasconcelos}

Enunciado: utilizando a base disponível no ​
\href{https://archive.ics.uci.edu/ml/datasets/Avila}{link}, crie os
datasets a seguir:

\begin{longtable}[]{@{}ll@{}}
\toprule
Dataset & \% de instâncias\tabularnewline
\midrule
\endhead
Treino & 60\%\tabularnewline
Validação & 20\%\tabularnewline
Teste & 20\%\tabularnewline
\bottomrule
\end{longtable}

Elabore uma rede neural de duas camadas para classificação do banco de
dados. Ao fim do treinamento, avalie o desempenho da rede utilizando a
matriz de confusão com o dataset de teste e mostre o valor de acurácia.
Observações:

● Utilize apenas o arquivo ​ avila-tr.txt​ .

● A camada de saída da rede deverá conter um neurônio para cada classe.

● Utilize o dataset de validação para criar algum critério de parada no
treinamento.

Bônus: defina uma arquitetura de rede neural ou modelos de ​ deep
learning que ultrapassem 75\% de acurácia.

    \subsubsection{DATA SET DESCRIPTION}\label{data-set-description}

The Avila data set has been extracted from 800 images of the the "Avila
Bible", a giant Latin copy of the whole Bible produced during the XII
century between Italy and Spain.\\
The palaeographic analysis of the manuscript has individuated the
presence of 12 copyists. The pages written by each copyist are not
equally numerous. Each pattern contains 10 features and corresponds to a
group of 4 consecutive rows.

The prediction task consists in associating each pattern to one of the
12 copyists (labeled as: A, B, C, D, E, F, G, H, I, W, X, Y). The data
have has been normalized, by using the Z-normalization method, and
divided in two data sets: a training set containing 10430 samples, and a
test set containing the 10437 samples.

\subsubsection{Class distribution (training
set)}\label{class-distribution-training-set}

\begin{itemize}
\tightlist
\item
  A: 4286
\item
  B: 5\\
\item
  C: 103
\item
  D: 352
\item
  E: 1095
\item
  F: 1961
\item
  G: 446
\item
  H: 519
\item
  I: 831
\item
  W: 44
\item
  X: 522
\item
  Y: 266
\end{itemize}

\subsubsection{ATTRIBUTE DESCRIPTION}\label{attribute-description}

\begin{longtable}[]{@{}ll@{}}
\toprule
ID & Name\tabularnewline
\midrule
\endhead
F1 & intercolumnar distance\tabularnewline
F2 & upper margin\tabularnewline
F3 & lower margin\tabularnewline
F4 & exploitation\tabularnewline
F5 & row number\tabularnewline
F6 & modular ratio\tabularnewline
F7 & interlinear spacing\tabularnewline
F8 & weight\tabularnewline
F9 & peak number\tabularnewline
F10 & modular ratio/ interlinear spacing\tabularnewline
\bottomrule
\end{longtable}

Class: A, B, C, D, E, F, G, H, I, W, X, Y

CITATIONS If you want to refer to the Avila data set in a publication,
please cite the following paper:

C. De~Stefano, M. Maniaci, F. Fontanella, A. Scotto~di~Freca, Reliable
writer identification in medieval manuscripts through page layout
features: The "Avila" Bible case, Engineering Applications of Artificial
Intelligence, Volume 72, 2018, pp. 99-110.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/nath/anaconda3/lib/python3.6/site-packages/h5py/\_\_init\_\_.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from .\_conv import register\_converters as \_register\_converters

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{dataset} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./avila/avila\PYZhy{}tr.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{header} \PY{o}{=} \PY{k+kc}{None}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{dataset}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:}          0         1         2         3         4         5         6   \textbackslash{}
        0  0.266074 -0.165620  0.320980  0.483299  0.172340  0.273364  0.371178   
        1  0.130292  0.870736 -3.210528  0.062493  0.261718  1.436060  1.465940   
        2 -0.116585  0.069915  0.068476 -0.783147  0.261718  0.439463 -0.081827   
        3  0.031541  0.297600 -3.210528 -0.583590 -0.721442 -0.307984  0.710932   
        4  0.229043  0.807926 -0.052442  0.082634  0.261718  0.148790  0.635431   
        
                 7         8         9  10  
        0  0.929823  0.251173  0.159345  A  
        1  0.636203  0.282354  0.515587  A  
        2 -0.888236 -0.123005  0.582939  A  
        3  1.051693  0.594169 -0.533994  A  
        4  0.051062  0.032902 -0.086652  F  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{dataset}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:}                   0             1             2             3             4  \textbackslash{}
        count  10430.000000  10430.000000  10430.000000  10430.000000  10430.000000   
        mean       0.000852      0.033611     -0.000525     -0.002387      0.006370   
        std        0.991431      3.920868      1.120202      1.008527      0.992053   
        min       -3.498799     -2.426761     -3.210528     -5.440122     -4.922215   
        25\%       -0.128929     -0.259834      0.064919     -0.528002      0.172340   
        50\%        0.043885     -0.055704      0.217845      0.095763      0.261718   
        75\%        0.204355      0.203385      0.352988      0.658210      0.261718   
        max       11.819916    386.000000     50.000000      3.987152      1.066121   
        
                          5             6             7             8             9  
        count  10430.000000  10430.000000  10430.000000  10430.000000  10430.000000  
        mean       0.013973      0.005605      0.010323      0.012914      0.000818  
        std        1.126245      1.313754      1.003507      1.087665      1.007094  
        min       -7.450257    -11.935457     -4.247781     -5.486218     -6.719324  
        25\%       -0.598658     -0.044076     -0.541992     -0.372457     -0.516097  
        50\%       -0.058835      0.220177      0.111803      0.064084     -0.034513  
        75\%        0.564038      0.446679      0.654944      0.500624      0.530855  
        max       53.000000     83.000000     13.173081     44.000000      4.671232  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} Aumentar o tamanho do plot na proporção 8/13}
        \PY{n}{x\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{17}
        \PY{n}{y\PYZus{}size} \PY{o}{=} \PY{n}{x\PYZus{}size} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{8}\PY{o}{/}\PY{l+m+mi}{13}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{n}{x\PYZus{}size}\PY{p}{,} \PY{n}{y\PYZus{}size}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Aumentar o tamanho do plot na proporção 8/13}
        
        \PY{n}{dataset}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7f0b945df2e8>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_8_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{\texorpdfstring{Percebendo o \emph{outlier} na coluna
1}{Percebendo o outlier na coluna 1}}\label{percebendo-o-outlier-na-coluna-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{a} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{386}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{a}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:}        0      1     2         3    4     5     6         7     8        9  10
        6619  0.0  386.0  50.0  0.168104  0.0  53.0  83.0  0.275032  44.0  0.63802  A
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{aux} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+m+mi}{386}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{nanmedian}\PY{p}{(}\PY{n}{aux}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{386}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} Empty DataFrame
         Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
         Index: []
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Aumentar o tamanho do plot na proporção 8/13}
         \PY{n}{x\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{17}
         \PY{n}{y\PYZus{}size} \PY{o}{=} \PY{n}{x\PYZus{}size} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{8}\PY{o}{/}\PY{l+m+mi}{13}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{n}{x\PYZus{}size}\PY{p}{,} \PY{n}{y\PYZus{}size}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Aumentar o tamanho do plot na proporção 8/13}
         \PY{n}{dataset}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7f0b92768e48>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{dataset}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:}                   0             1             2             3             4  \textbackslash{}
         count  10430.000000  10430.000000  10430.000000  10430.000000  10430.000000   
         mean       0.000852     -0.003403     -0.000525     -0.002387      0.006370   
         std        0.991431      1.042894      1.120202      1.008527      0.992053   
         min       -3.498799     -2.426761     -3.210528     -5.440122     -4.922215   
         25\%       -0.128929     -0.259834      0.064919     -0.528002      0.172340   
         50\%        0.043885     -0.055704      0.217845      0.095763      0.261718   
         75\%        0.204355      0.203385      0.352988      0.658210      0.261718   
         max       11.819916     43.133656     50.000000      3.987152      1.066121   
         
                           5             6             7             8             9  
         count  10430.000000  10430.000000  10430.000000  10430.000000  10430.000000  
         mean       0.013973      0.005605      0.010323      0.012914      0.000818  
         std        1.126245      1.313754      1.003507      1.087665      1.007094  
         min       -7.450257    -11.935457     -4.247781     -5.486218     -6.719324  
         25\%       -0.598658     -0.044076     -0.541992     -0.372457     -0.516097  
         50\%       -0.058835      0.220177      0.111803      0.064084     -0.034513  
         75\%        0.564038      0.446679      0.654944      0.500624      0.530855  
         max       53.000000     83.000000     13.173081     44.000000      4.671232  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{b} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{b}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} array([ 0.371178,  1.46594 , -0.081827, {\ldots},  0.295677,  0.069175,
                 0.786433])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{b}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} 83.0
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{:}
             \PY{n}{aux} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
             \PY{n}{dataset}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{aux}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{nanmedian}\PY{p}{(}\PY{n}{aux}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{dataset}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:}                   0             1             2             3             4  \textbackslash{}
         count  10430.000000  10430.000000  10430.000000  10430.000000  10430.000000   
         mean       0.000852     -0.007544     -0.005298     -0.002387      0.006370   
         std        0.991431      0.953512      1.007528      1.008527      0.992053   
         min       -3.498799     -2.426761     -3.210528     -5.440122     -4.922215   
         25\%       -0.128929     -0.259834      0.064919     -0.528002      0.172340   
         50\%        0.043885     -0.055704      0.217845      0.095763      0.261718   
         75\%        0.204355      0.203385      0.352988      0.658210      0.261718   
         max       11.819916     19.470188      7.458681      3.987152      1.066121   
         
                           5             6             7             8             9  
         count  10430.000000  10430.000000  10430.000000  10430.000000  10430.000000  
         mean       0.008886     -0.002331      0.009070      0.008702      0.000818  
         std        0.999600      1.032191      0.995195      0.998735      1.007094  
         min       -7.450257    -11.935457     -4.247781     -5.486218     -6.719324  
         25\%       -0.598658     -0.044076     -0.541992     -0.372457     -0.516097  
         50\%       -0.058835      0.220177      0.111778      0.064084     -0.034513  
         75\%        0.564038      0.446679      0.654886      0.500624      0.530855  
         max        5.505495     10.714792      4.510897      3.244594      4.671232  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} Aumentar o tamanho do plot na proporção 8/13}
         \PY{n}{x\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{17}
         \PY{n}{y\PYZus{}size} \PY{o}{=} \PY{n}{x\PYZus{}size} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{8}\PY{o}{/}\PY{l+m+mi}{13}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{n}{x\PYZus{}size}\PY{p}{,} \PY{n}{y\PYZus{}size}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Aumentar o tamanho do plot na proporção 8/13}
         \PY{n}{dataset}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7f0b91db00f0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Mudando a classe de caractere para
inteiro}\label{mudando-a-classe-de-caractere-para-inteiro}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{classes\PYZus{}number} \PY{o}{=} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{]}
         \PY{n}{classes\PYZus{}charcteres} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{B}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{D}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{E}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{G}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{H}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{I}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{dictionary\PYZus{}classes} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{classes\PYZus{}charcteres}\PY{p}{,} \PY{n}{classes\PYZus{}number}\PY{p}{)}\PY{p}{)}
         \PY{n}{dictionary\PYZus{}classes}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}20}]:} \{'A': 0,
          'B': 1,
          'C': 2,
          'D': 3,
          'E': 4,
          'F': 5,
          'G': 6,
          'H': 7,
          'I': 8,
          'W': 9,
          'X': 10,
          'Y': 11\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:} 0    A
         1    A
         2    A
         3    A
         4    F
         Name: 10, dtype: object
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{dictionary\PYZus{}classes}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{key}\PY{p}{,} \PY{n}{dictionary\PYZus{}classes}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)}
             \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{key}\PY{p}{,} \PY{n}{dictionary\PYZus{}classes}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A 0
B 1
C 2
D 3
E 4
F 5
G 6
H 7
I 8
W 9
X 10
Y 11

    \end{Verbatim}

    Testando se realmente funcionou

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:} 0    0
         1    0
         2    0
         3    0
         4    5
         Name: 10, dtype: int64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{tail}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:} 10425     5
         10426     5
         10427     0
         10428     4
         10429    10
         Name: 10, dtype: int64
\end{Verbatim}
            
    \section{Análise inicial de correlação entre os
atributos}\label{anuxe1lise-inicial-de-correlauxe7uxe3o-entre-os-atributos}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{matshow}\PY{p}{(}\PY{n}{dataset}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}25}]:} <matplotlib.image.AxesImage at 0x7f0b8df0b630>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Percebemos que os atributos 5('row number') e 9(peak number) possuem
alta correlação

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{9}\PY{p}{]}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}26}]:} 0.8485534155934017
\end{Verbatim}
            
    ?dataset.plot('hist')

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{a} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{subplots}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{21}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{21}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{,} \PY{n}{xticks}\PY{o}{=}\PY{p}{[}\PY{n}{i}\PY{o}{/}\PY{l+m+mi}{2} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{24}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}28}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7f0b8c7832e8>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Normalizar os atributos}\label{normalizar-os-atributos}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{dataset}\PY{o}{.}\PY{n}{columns}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}29}]:} Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype='int64')
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}
         
         \PY{n}{X} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{n}{y} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{n}{min\PYZus{}max\PYZus{}scaler} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}
         \PY{n}{x\PYZus{}scaled} \PY{o}{=} \PY{n}{min\PYZus{}max\PYZus{}scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{x\PYZus{}scaled}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{df}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}31}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7f0b8c4ac358>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_39_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{n}{dataset}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}32}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7f0b7c200710>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_40_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{dataset}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}33}]:}              0         1         2         3         4         5         6   \textbackslash{}
         0      0.266074 -0.165620  0.320980  0.483299  0.172340  0.273364  0.371178   
         1      0.130292  0.870736 -3.210528  0.062493  0.261718  1.436060  1.465940   
         2     -0.116585  0.069915  0.068476 -0.783147  0.261718  0.439463 -0.081827   
         3      0.031541  0.297600 -3.210528 -0.583590 -0.721442 -0.307984  0.710932   
         4      0.229043  0.807926 -0.052442  0.082634  0.261718  0.148790  0.635431   
         5      0.117948 -0.220579 -3.210528 -1.623238  0.261718 -0.349509  0.257927   
         6      0.389513 -0.220579 -3.210528 -2.624155  0.261718 -0.764757  0.484429   
         7      0.019197 -0.040001  0.288973 -0.042597  0.261718 -1.013906  0.069175   
         8      0.500607  0.140576  0.388552 -0.637358  0.261718 -0.681707  0.295677   
         9     -0.252367  0.069915  0.246296  0.523550  0.261718 -1.221530  0.899684   
         10    -0.042522 -2.395356 -3.210528 -1.128463 -0.721442  0.397939  0.257927   
         11    -3.498799 -0.566031  0.139604 -5.440122  0.976743 -0.847807 -1.176589   
         12     0.043885 -0.173471  0.285416 -0.775121  0.172340 -0.391033  0.333428   
         13     0.154980  0.336855  0.068476  0.021525  0.261718  0.024215  0.031425   
         14     0.290762 -0.189174  0.079145 -0.085921  0.976743 -1.470679 -0.950086   
         15     0.142636 -0.244132 -3.206971  0.165524  0.887365 -2.093552 -0.874585   
         16    -0.005490 -0.322644  0.100483 -0.519439  0.261718 -0.307984  0.069175   
         17    -0.301743 -0.314793  0.399221  0.770520  0.708609  0.564038 -1.327590   
         18    -0.091897  0.297600  0.079145  0.196496  0.261718 -0.183409  0.220177   
         19    -0.091897 -0.220579  0.274747  0.567174 -0.185173  0.730137  0.031425   
         20    -0.005490  0.478177  0.029355 -0.247644  0.172340  1.062336  0.333428   
         21     0.438888  0.195534  0.143160 -0.809435  0.261718 -1.138481 -0.232828   
         22     0.364825 -0.047852 -0.038216  0.327813  0.261718  0.439463  0.484429   
         23    -0.054866 -0.220579  0.466793 -0.216970  0.172340 -0.930856  0.446679   
         24     0.290762  0.077766  0.118265  0.275805  0.261718 -0.432558  0.031425   
         25     0.105604 -0.087108  0.367214  1.522618  0.261718  1.186911  0.635431   
         26    -0.030178  0.022808  0.157386 -0.771451  0.261718  1.062336  0.182426   
         27    -0.252367 -0.008597 -2.044028  0.342613  0.351096 -0.307984  0.220177   
         28     0.290762 -0.189174  0.079145 -0.085921  0.976743 -0.930856 -1.101088   
         29     0.253730  0.336855  0.086258 -0.979571  0.261718 -1.055431  0.144676   
         {\ldots}         {\ldots}       {\ldots}       {\ldots}       {\ldots}       {\ldots}       {\ldots}       {\ldots}   
         10400  0.142636  0.486028  0.100483 -0.448543  0.172340  1.519109  0.597681   
         10401  0.192011 -0.369751  0.278304 -0.659618  0.261718  1.062336  0.182426   
         10402 -0.375806 -0.291239 -3.210528 -0.835274  0.082961 -0.889332 -2.950858   
         10403  0.043885 -0.079257  0.061363  0.565085  0.261718  1.145386  0.408929   
         10404 -0.079554 -0.338346  0.409890  0.129748  0.172340 -0.058835  0.182426   
         10405 -0.091897 -0.142067  0.324537  0.658210  0.172340 -0.307984  0.295677   
         10406 -0.178304  0.014957  0.264078  0.582361  0.172340  0.439463  0.144676   
         10407  0.229043  0.124874  0.182281  0.686496  0.261718  2.889429  0.748682   
         10408 -0.314087 -0.024299 -1.681275  0.307284  0.261718  0.148790  0.182426   
         10409 -2.523635 -0.346197 -2.349878 -0.775688  0.172340 -0.474083  0.371178   
         10410 -0.079554 -0.338346  0.409890  0.129748  0.172340 -0.640182 -0.081827   
         10411 -0.116585 -0.079257  0.381439  1.540879  0.172340  0.564038  0.069175   
         10412 -0.289399  0.831480  0.157386  0.987562  0.529852 -0.432558  1.088436   
         10413  0.241386 -0.110662  0.324537 -0.772226  0.261718  0.397939  0.257927   
         10414  0.130292 -0.071406  0.214288  0.915742  0.261718  0.730137  0.786433   
         10415 -0.264711 -0.126364  0.235627  0.176773  0.261718  0.564038  0.182426   
         10416  0.130292  0.870736 -3.210528  0.062493  0.261718  0.190314  0.257927   
         10417  0.204355  0.360409  0.139604  0.299019 -0.095795  2.141982  0.710932   
         10418 -0.042522 -0.189174  0.331650 -0.664392  0.261718 -1.512204  0.069175   
         10419 -0.474557  0.446772 -3.210528 -0.716161  0.351096  4.550423  0.522180   
         10420 -0.005490  0.478177  0.029355 -0.247644  0.172340  0.605563  0.673182   
         10421  0.241386  0.234790  0.121822  1.037988  0.261718  0.647088  0.182426   
         10422 -0.277055 -0.251983 -3.203415  1.957926  0.261718  1.892833  0.635431   
         10423  4.969080 -0.385453  0.143160 -2.600732  0.976743 -0.764757 -0.232828   
         10424  0.216699  0.321153  0.128935  0.491087  0.261718  0.439463  0.069175   
         10425  0.080916  0.588093  0.015130  0.002250  0.261718 -0.557133  0.371178   
         10426  0.253730 -0.338346  0.352988 -1.154243  0.172340 -0.557133  0.257927   
         10427  0.229043 -0.000745  0.171611 -0.002793  0.261718  0.688613  0.295677   
         10428 -0.301743  0.352558  0.288973  1.638181  0.261718  0.688613  0.069175   
         10429 -0.104241 -1.037102  0.388552 -1.099311  0.172340 -0.307984  0.786433   
         
                      7         8         9   10  
         0      0.929823  0.251173  0.159345   0  
         1      0.636203  0.282354  0.515587   0  
         2     -0.888236 -0.123005  0.582939   0  
         3      1.051693  0.594169 -0.533994   0  
         4      0.051062  0.032902 -0.086652   5  
         5     -0.385979 -0.247731 -0.331310   0  
         6     -0.597510 -0.372457 -0.810261   0  
         7      0.890701  0.095265 -0.842014   5  
         8      0.931046  0.500624 -0.642297   7  
         9      1.373076  0.625350 -1.400890   4  
         10    -1.167255 -0.060642  0.345537   0  
         11    -2.008078  0.438262  0.009512   8  
         12    -1.807711 -0.996086 -0.410850   5  
         13     0.079292  0.313536  0.125675   0  
         14    -0.273525  0.968347 -0.724959   8  
         15    -2.694490 -3.334697 -1.399407   8  
         16    -0.574114  0.282354 -0.182852   0  
         17     0.004193  0.750076  1.649058  11  
         18     0.305093 -0.278912 -0.163443   5  
         19    -0.396638 -0.403638  0.770625   0  
         20    -0.670454  0.095265  0.902376   0  
         21    -0.087894 -0.278912 -0.802015   5  
         22    -0.964306 -0.933723  0.232267   0  
         23     0.634922  0.313536 -0.942674   3  
         24     0.676859 -0.091823 -0.293988   5  
         25     0.574967  0.656532  0.817580   0  
         26    -1.217230 -0.185368  0.981553   7  
         27     1.240469  2.090879 -0.295362  10  
         28     0.495694  1.685520 -0.087857   8  
         29    -0.626478  0.064084 -0.896238   6  
         {\ldots}         {\ldots}       {\ldots}       {\ldots}  ..  
         10400 -0.779685 -1.276719  1.123020   0  
         10401 -1.218279 -0.466001  0.978009   5  
         10402 -0.238674  0.001721 -2.199566   3  
         10403 -0.197785 -0.216549  0.914816   0  
         10404  0.283464  0.531806 -0.039143   0  
         10405 -0.168119 -0.840179 -0.324222   5  
         10406  1.025657  0.126447  0.442665   0  
         10407 -1.313899 -0.777816  2.230284   0  
         10408  0.524060  0.219991  0.142276   4  
         10409  0.574559 -0.029461 -0.505543   5  
         10410  0.634669  0.500624 -0.421122   0  
         10411  0.818883  1.654339  0.660936  10  
         10412 -0.421470 -0.123005 -0.075416  11  
         10413 -1.689549 -0.559546  0.334759   5  
         10414  0.006037 -0.185368  0.329010   0  
         10415 -0.183534 -0.933723  0.529293   0  
         10416  1.605517  0.750076  0.150569   0  
         10417 -1.136773 -0.653090  1.605636   0  
         10418  0.063663  1.030710 -1.295118   7  
         10419 -2.037065 -2.368071  3.871867   0  
         10420 -0.951919 -0.528364  0.286973   0  
         10421  0.684936  0.219991  0.628422   0  
         10422  1.898205  2.184424  1.427425  10  
         10423 -2.348488 -1.183175 -0.459372   8  
         10424  0.252846  0.188810  0.482857   0  
         10425  0.932346  0.282354 -0.580141   5  
         10426  0.348428  0.032902 -0.527134   5  
         10427 -1.088486 -0.590727  0.580142   0  
         10428  0.502761  0.625350  0.718969   4  
         10429 -1.337547  0.999528 -0.551063  10  
         
         [10430 rows x 11 columns]
\end{Verbatim}
            
    \section{Diminuir a classe A do
sistema}\label{diminuir-a-classe-a-do-sistema}

Devido a um desbalanceamento das classes é necessário remover algumas
instancias de A

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{resample}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{dataset\PYZus{}b} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}


    \section{Criando fake data para a classe
B}\label{criando-fake-data-para-a-classe-b}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} count    10430.000000
         mean         3.542761
         std          3.418046
         min          0.000000
         25\%          0.000000
         50\%          4.000000
         75\%          6.000000
         max         11.000000
         Name: 10, dtype: float64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{c+c1}{\PYZsh{} Docstring:}
         \PY{c+c1}{\PYZsh{} Return random integer in range [a, b], including both end points.}
         \PY{n}{df} \PY{o}{=} \PY{n}{dataset}
         \PY{c+c1}{\PYZsh{} array com indices de B}
         \PY{n}{a} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{dataset}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{index}
         \PY{n}{a}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}37}]:} Int64Index([708, 4639, 7119, 7740, 9457], dtype='int64')
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{k+kn}{import} \PY{n+nn}{random}
         \PY{k+kn}{import} \PY{n+nn}{datetime}
         \PY{k+kn}{from} \PY{n+nn}{datetime} \PY{k}{import} \PY{n}{datetime} \PY{k}{as} \PY{n}{datetime}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
             \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{n}{parametro\PYZus{}alterado} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{9}\PY{p}{)}
             \PY{n}{linha\PYZus{}alterada} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{a}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{valor\PYZus{}a\PYZus{}ser\PYZus{}alterado} \PY{o}{=} \PY{p}{(}\PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{10}
             \PY{n}{soma\PYZus{}ou\PYZus{}diminui} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{parametro\PYZus{}alterado}\PY{p}{,} \PY{n}{linha\PYZus{}alterada}\PY{p}{,} \PY{n}{valor\PYZus{}a\PYZus{}ser\PYZus{}alterado}\PY{p}{,} \PY{n}{soma\PYZus{}ou\PYZus{}diminui}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{parametro\PYZus{}alterado}\PY{p}{]}\PY{p}{[}\PY{n}{a}\PY{p}{[}\PY{n}{linha\PYZus{}alterada}\PY{p}{]}\PY{p}{]}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{valor\PYZus{}a\PYZus{}ser\PYZus{}alterado} \PY{o}{+} \PY{n}{df}\PY{p}{[}\PY{n}{parametro\PYZus{}alterado}\PY{p}{]}\PY{p}{[}\PY{n}{a}\PY{p}{[}\PY{n}{linha\PYZus{}alterada}\PY{p}{]}\PY{p}{]}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{linha\PYZus{}alterada}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{n}{linha\PYZus{}alterada}\PY{p}{]}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+m+mi}{12}\PY{o}{*}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             
             
             
             \PY{k}{if} \PY{n}{soma\PYZus{}ou\PYZus{}diminui} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                 \PY{k}{pass}
                 \PY{c+c1}{\PYZsh{}soma}
                 \PY{c+c1}{\PYZsh{}df.append(pd.DataFrame(valor\PYZus{}a\PYZus{}ser\PYZus{}alterado + df[parametro\PYZus{}alterado][a[linha\PYZus{}alterada]]))}
             
             \PY{c+c1}{\PYZsh{}else:}
             \PY{c+c1}{\PYZsh{}    df = df.append(\PYZhy{}valor\PYZus{}a\PYZus{}ser\PYZus{}alterado + df[parametro\PYZus{}alterado][a[linha\PYZus{}alterada]])}
             
             \PY{c+c1}{\PYZsh{}a = df.loc[dataset[10] == 1].index}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0 4 0.09190188811913438 1
-0.128929
-0.037027111880865604
         0       1         2        3         4         5         6   \textbackslash{}
3  0.031541  0.2976 -3.210528 -0.58359 -0.721442 -0.307984  0.710932   

         7         8         9   10  
3  1.051693  0.594169 -0.533994   0  
\#\#\#\#\#\#\#\#\#\#\#\#
4 4 0.03995625978983738 1
-3.22403
-3.1840737402101627
         0       1         2        3         4         5         6   \textbackslash{}
3  0.031541  0.2976 -3.210528 -0.58359 -0.721442 -0.307984  0.710932   

         7         8         9   10  
3  1.051693  0.594169 -0.533994   0  
\#\#\#\#\#\#\#\#\#\#\#\#
8 2 0.023568265700846015 0
1.404887
1.428455265700846
         0         1         2         3         4        5        6   \textbackslash{}
1  0.130292  0.870736 -3.210528  0.062493  0.261718  1.43606  1.46594   

         7         8         9   10  
1  0.636203  0.282354  0.515587   0  
\#\#\#\#\#\#\#\#\#\#\#\#
7 0 0.08392281545422367 1
0.80704
0.8909628154542236
Empty DataFrame
Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Index: []
\#\#\#\#\#\#\#\#\#\#\#\#
8 1 0.05496091420794534 1
1.903791
1.9587519142079453
         0        1        2         3        4         5         6   \textbackslash{}
0  0.266074 -0.16562  0.32098  0.483299  0.17234  0.273364  0.371178   

         7         8         9   10  
0  0.929823  0.251173  0.159345   0  
\#\#\#\#\#\#\#\#\#\#\#\#
1 2 0.03664369863667409 0
12.655362
12.692005698636674
         0         1         2         3         4        5        6   \textbackslash{}
1  0.130292  0.870736 -3.210528  0.062493  0.261718  1.43606  1.46594   

         7         8         9   10  
1  0.636203  0.282354  0.515587   0  
\#\#\#\#\#\#\#\#\#\#\#\#
8 0 0.09870835870226205 0
1.186617
1.2853253587022622
Empty DataFrame
Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Index: []
\#\#\#\#\#\#\#\#\#\#\#\#
1 4 0.06968034335899059 1
12.655362
12.725042343358991
         0       1         2        3         4         5         6   \textbackslash{}
3  0.031541  0.2976 -3.210528 -0.58359 -0.721442 -0.307984  0.710932   

         7         8         9   10  
3  1.051693  0.594169 -0.533994   0  
\#\#\#\#\#\#\#\#\#\#\#\#
1 3 0.09080262265097096 1
12.655362
12.746164622650971
         0         1         2         3         4         5         6   \textbackslash{}
2 -0.116585  0.069915  0.068476 -0.783147  0.261718  0.439463 -0.081827   

         7         8         9   10  
2 -0.888236 -0.123005  0.582939   0  
\#\#\#\#\#\#\#\#\#\#\#\#
9 4 0.09427459555915263 0
0.9462200000000001
1.0404945955591527
         0       1         2        3         4         5         6   \textbackslash{}
3  0.031541  0.2976 -3.210528 -0.58359 -0.721442 -0.307984  0.710932   

         7         8         9   10  
3  1.051693  0.594169 -0.533994   0  
\#\#\#\#\#\#\#\#\#\#\#\#

    \end{Verbatim}

    \section{Usando Keras}\label{usando-keras}

    Keras é uma API de alto nível para o tensorflow, que permite criar redes
neurais complexas sem precisar entender as variáveis que o Tensorflow
possui

    Primeiro é preciso particionar a base de acordo com as porcentagens
abaixo

\begin{longtable}[]{@{}ll@{}}
\toprule
Dataset & \% de instâncias\tabularnewline
\midrule
\endhead
Treino & 60\%\tabularnewline
Validação & 20\%\tabularnewline
Teste & 20\%\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{X} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{n}{y} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{k+kn}{import} \PY{n+nn}{math}
\end{Verbatim}


    Pega os valores do começo até o numero correspondente a 60\% menos 1
(função floor)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{n}{x\PYZus{}treinamento} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{n}{math}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{p}{]}
         \PY{n}{y\PYZus{}treinamento} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{p}{:}\PY{n}{math}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    Pega os valores do número correspondente a 60\% mais 1 (função ceil) até
60\% mais 1 (função ceil) mais 20\% menos 1 (função floor)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{x\PYZus{}validacao} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{p}{:}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{o}{+}\PY{n}{math}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         \PY{n}{y\PYZus{}validacao} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{p}{:}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{o}{+}\PY{n}{math}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    Pega os valores do número correspondente a 60\% mais 1 (função ceil)
mais 20\% mais 1 (função ceil) mais 20\% menos 1 função floor

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{x\PYZus{}teste} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{o}{+}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}\PY{p}{:}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{o}{+}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{o}{+}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         \PY{n}{y\PYZus{}teste} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{o}{+}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}\PY{p}{:}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.6}\PY{p}{)}\PY{o}{+}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{o}{+}\PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    Pega os valores do número correspondente a 60\% mais 1 (função ceil) até
60\% mais 1 (função ceil) mais 20\% mais 1 (função floor)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{softmax}\PY{p}{)}
         \PY{p}{]}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{GradientDescentOptimizer}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
                       \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{model.compile(optimizer=\PYZsq{}adam\PYZsq{},}
         \PY{l+s+sd}{              loss=\PYZsq{}sparse\PYZus{}categorical\PYZus{}crossentropy\PYZsq{},}
         \PY{l+s+sd}{              metrics=[\PYZsq{}accuracy\PYZsq{}])}
         \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}treinamento}\PY{p}{,} \PY{n}{y\PYZus{}treinamento}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Evaluate:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Returns the loss value \PYZam{} metrics values for the model in test mode.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{x\PYZus{}teste}\PY{p}{,} \PY{n}{y\PYZus{}teste}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Returns the loss value \PYZam{} metrics values for the model in test mode. Test using validation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{x\PYZus{}validacao}\PY{p}{,} \PY{n}{y\PYZus{}validacao}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/20
6258/6258 [==============================] - 0s 56us/step - loss: 1.7228 - acc: 0.4578
Epoch 2/20
6258/6258 [==============================] - 0s 21us/step - loss: 1.3822 - acc: 0.5297
Epoch 3/20
6258/6258 [==============================] - 0s 20us/step - loss: 1.2863 - acc: 0.5641
Epoch 4/20
6258/6258 [==============================] - 0s 20us/step - loss: 1.2286 - acc: 0.5844
Epoch 5/20
6258/6258 [==============================] - 0s 21us/step - loss: 1.1793 - acc: 0.6056
Epoch 6/20
6258/6258 [==============================] - 0s 20us/step - loss: 1.1390 - acc: 0.6179
Epoch 7/20
6258/6258 [==============================] - 0s 20us/step - loss: 1.1062 - acc: 0.6261
Epoch 8/20
6258/6258 [==============================] - 0s 20us/step - loss: 1.0802 - acc: 0.6318
Epoch 9/20
6258/6258 [==============================] - 0s 19us/step - loss: 1.0571 - acc: 0.6384
Epoch 10/20
6258/6258 [==============================] - 0s 21us/step - loss: 1.0388 - acc: 0.6406
Epoch 11/20
6258/6258 [==============================] - 0s 22us/step - loss: 1.0233 - acc: 0.6483
Epoch 12/20
6258/6258 [==============================] - 0s 21us/step - loss: 1.0090 - acc: 0.6494
Epoch 13/20
6258/6258 [==============================] - 0s 22us/step - loss: 1.0029 - acc: 0.6505
Epoch 14/20
6258/6258 [==============================] - 0s 22us/step - loss: 0.9898 - acc: 0.6582
Epoch 15/20
6258/6258 [==============================] - 0s 20us/step - loss: 0.9827 - acc: 0.6572
Epoch 16/20
6258/6258 [==============================] - 0s 21us/step - loss: 0.9712 - acc: 0.6595
Epoch 17/20
6258/6258 [==============================] - 0s 23us/step - loss: 0.9649 - acc: 0.6641
Epoch 18/20
6258/6258 [==============================] - 0s 25us/step - loss: 0.9535 - acc: 0.6655
Epoch 19/20
6258/6258 [==============================] - 0s 19us/step - loss: 0.9529 - acc: 0.6681
Epoch 20/20
6258/6258 [==============================] - 0s 21us/step - loss: 0.9418 - acc: 0.6697
Evaluate:
Returns the loss value \& metrics values for the model in test mode.
2086/2086 [==============================] - 0s 32us/step
[0.9191266548599287, 0.6706615531547415]
Returns the loss value \& metrics values for the model in test mode. Test using validation
2086/2086 [==============================] - 0s 12us/step
[0.9578683657477023, 0.6596356664042231]

    \end{Verbatim}

    Errado, pois a separação está diferente da tabela acima, foi usada uma
nova porcentagem para testes

\begin{longtable}[]{@{}ll@{}}
\toprule
Dataset & \% de instâncias\tabularnewline
\midrule
\endhead
Treino & 60\%\tabularnewline
Teste & 40\%\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{n}{model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{softmax}\PY{p}{)}
         \PY{p}{]}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Evaluate:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Returns the loss value \PYZam{} metrics values for the model in test mode.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/20
6258/6258 [==============================] - 1s 106us/step - loss: 1.6251 - acc: 0.4765
Epoch 2/20
6258/6258 [==============================] - 0s 38us/step - loss: 1.2016 - acc: 0.5719
Epoch 3/20
6258/6258 [==============================] - 0s 39us/step - loss: 1.0530 - acc: 0.6254
Epoch 4/20
6258/6258 [==============================] - 0s 38us/step - loss: 0.9627 - acc: 0.6548
Epoch 5/20
6258/6258 [==============================] - 0s 39us/step - loss: 0.8949 - acc: 0.6729
Epoch 6/20
6258/6258 [==============================] - 0s 38us/step - loss: 0.8473 - acc: 0.6833
Epoch 7/20
6258/6258 [==============================] - 0s 39us/step - loss: 0.8087 - acc: 0.6972
Epoch 8/20
6258/6258 [==============================] - 0s 38us/step - loss: 0.7795 - acc: 0.7133
Epoch 9/20
6258/6258 [==============================] - 0s 40us/step - loss: 0.7458 - acc: 0.7202
Epoch 10/20
6258/6258 [==============================] - 0s 40us/step - loss: 0.7226 - acc: 0.7263
Epoch 11/20
6258/6258 [==============================] - 0s 41us/step - loss: 0.6907 - acc: 0.7331
Epoch 12/20
6258/6258 [==============================] - 0s 40us/step - loss: 0.6761 - acc: 0.7408
Epoch 13/20
6258/6258 [==============================] - 0s 41us/step - loss: 0.6506 - acc: 0.7469
Epoch 14/20
6258/6258 [==============================] - 0s 40us/step - loss: 0.6417 - acc: 0.7502
Epoch 15/20
6258/6258 [==============================] - 0s 41us/step - loss: 0.6226 - acc: 0.7558
Epoch 16/20
6258/6258 [==============================] - 0s 40us/step - loss: 0.6003 - acc: 0.7685
Epoch 17/20
6258/6258 [==============================] - 0s 42us/step - loss: 0.5884 - acc: 0.7661
Epoch 18/20
6258/6258 [==============================] - 0s 42us/step - loss: 0.5691 - acc: 0.7702
Epoch 19/20
6258/6258 [==============================] - 0s 43us/step - loss: 0.5449 - acc: 0.7803
Epoch 20/20
6258/6258 [==============================] - 0s 42us/step - loss: 0.5330 - acc: 0.7903
Evaluate:
Returns the loss value \& metrics values for the model in test mode.
4172/4172 [==============================] - 0s 44us/step

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}52}]:} [0.631493254338792, 0.7615052731930924]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{n}{model} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{relu}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
           \PY{n}{tf}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{softmax}\PY{p}{)}
         \PY{p}{]}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{GradientDescentOptimizer}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{,}
                       \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Evaluate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Returns the loss value \PYZam{} metrics values for the model in test mode.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/20
6258/6258 [==============================] - 0s 76us/step - loss: 1.5845 - acc: 0.4848
Epoch 2/20
6258/6258 [==============================] - 0s 31us/step - loss: 1.2148 - acc: 0.5722
Epoch 3/20
6258/6258 [==============================] - 0s 31us/step - loss: 1.1045 - acc: 0.6026
Epoch 4/20
6258/6258 [==============================] - 0s 32us/step - loss: 1.0166 - acc: 0.6331
Epoch 5/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.9732 - acc: 0.6465
Epoch 6/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.9185 - acc: 0.6657
Epoch 7/20
6258/6258 [==============================] - 0s 31us/step - loss: 0.8870 - acc: 0.6721
Epoch 8/20
6258/6258 [==============================] - 0s 29us/step - loss: 0.8614 - acc: 0.6831
Epoch 9/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.8309 - acc: 0.6956
Epoch 10/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.8052 - acc: 0.6985
Epoch 11/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.7809 - acc: 0.7052
Epoch 12/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.7575 - acc: 0.7183
Epoch 13/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.7318 - acc: 0.7215
Epoch 14/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.7188 - acc: 0.7202
Epoch 15/20
6258/6258 [==============================] - 0s 31us/step - loss: 0.6946 - acc: 0.7320
Epoch 16/20
6258/6258 [==============================] - 0s 29us/step - loss: 0.6698 - acc: 0.7367
Epoch 17/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.6543 - acc: 0.7391
Epoch 18/20
6258/6258 [==============================] - 0s 30us/step - loss: 0.6320 - acc: 0.7459
Epoch 19/20
6258/6258 [==============================] - 0s 29us/step - loss: 0.6205 - acc: 0.7546
Epoch 20/20
6258/6258 [==============================] - 0s 31us/step - loss: 0.6090 - acc: 0.7584
Evaluate
Returns the loss value \& metrics values for the model in test mode.
4172/4172 [==============================] - 0s 38us/step

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}53}]:} [0.7204558540739241, 0.72315436235896]
\end{Verbatim}
            

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
